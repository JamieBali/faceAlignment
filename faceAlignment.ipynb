{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceAlignment",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOf4FeSdPldSfZqmAPkqhf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamieBali/faceAlignment/blob/main/faceAlignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18boeopluMvo"
      },
      "source": [
        "# Prerequisites "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZiJ8Pw5V_w2"
      },
      "source": [
        "!pip uninstall opencv-python -y\n",
        "!pip install opencv-contrib-python==3.4.2.17 --force-reinstall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCTVxjj2ruJG"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import linear_model\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1LoSrN78HP6",
        "outputId": "0fc564ca-421e-40cd-a23a-5b9257633894"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i18dYKA-_OzN"
      },
      "source": [
        "\n",
        "# Download the data stored in a zipped numpy array from one of these two locations\n",
        "# The uncommented one is likely to be faster. If you're running all your experiments\n",
        "# on a machine at home rather than using colab, then make sure you save it \n",
        "# rather than repeatedly downloading it.\n",
        "!wget \"http://users.sussex.ac.uk/~is321/training_images.npz\" -O training_images.npz\n",
        "# The test images (without points)\n",
        "!wget \"http://users.sussex.ac.uk/~is321/test_images.npz\" -O test_images.npz\n",
        "# The example images are here\n",
        "!wget \"http://users.sussex.ac.uk/~is321/examples.npz\" -O examples.npz\n",
        "\n",
        "# Load the data using np.load\n",
        "training_data = np.load('training_images.npz', allow_pickle=True)\n",
        "test_data = np.load('test_images.npz', allow_pickle=True)\n",
        "example_data = np.load('examples.npz', allow_pickle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlW44s7lwBg2"
      },
      "source": [
        "\n",
        "# Extract the images\n",
        "training_images = training_data['images']\n",
        "test_images = test_data['images']\n",
        "example_images = example_data['images']\n",
        "# and the data points\n",
        "training_pts = training_data['points']\n",
        "\n",
        "training_set = training_images[:-100]     # all but the last 100 for training\n",
        "training_set_pts = training_pts[:-100]\n",
        "\n",
        "validation_set = training_images[-100:]   # the last 100 for validation\n",
        "validation_set_points = training_pts[-100:]\n",
        "\n",
        "## have a look at whether using 100 has a significant effect? It's like 2700 other images, so it should be fine but uhhhhhhhhh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1cs0YcMzBc-"
      },
      "source": [
        "visualise_pts(training_set[-1], training_set_pts[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwG7zKjEQQVL",
        "outputId": "1f8805f2-c8da-4cbc-bff1-338c80886fd8"
      },
      "source": [
        "cumulative = []\n",
        "for x in training_set_pts:\n",
        "  cumulative.append(np.average(calculate_error(avg_pts, x)))\n",
        "\n",
        "print(np.average(cumulative))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.84346871689841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CruHd2puURs"
      },
      "source": [
        "# Reusable Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKxEJcZStOkj"
      },
      "source": [
        "def visualise_pts(img, pts, back=None):\n",
        "  plt.imshow(img, back)\n",
        "  for x in pts:\n",
        "    plt.plot(x[0],x[1], '+r')\n",
        "  plt.show()\n",
        "\n",
        "def calculate_error(pred_pts, gt_pts):\n",
        "  \"\"\"\n",
        "  Calculate the euclidean distance between pairs of points\n",
        "  :param pred_pts: The predicted points\n",
        "  :param gt_pts: The ground truth points\n",
        "  :return: An array of shape (no_points,) containing the distance of each predicted point from the ground truth\n",
        "  \"\"\"\n",
        "  pred_pts = np.reshape(pred_pts, (-1, 2))\n",
        "  gt_pts = np.reshape(gt_pts, (-1, 2))\n",
        "  return np.sqrt(np.sum(np.square(pred_pts - gt_pts), axis=-1))  \n",
        "\n",
        "def save_as_csv(points, location = '.'):\n",
        "  \"\"\"\n",
        "  Save the points out as a .csv file\n",
        "  :param points: numpy array of shape (no_image, no_points, 2) to be saved\n",
        "  :param location: Directory to save results.csv in. Default to current working directory\n",
        "  \"\"\"\n",
        "  np.savetxt(location + '/results.csv', np.reshape(points, (points.shape[0], -1)), delimiter=',')\n",
        "\n",
        "\n",
        "visualise_pts(training_images[0], training_pts[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k45SozVyLDA"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1nTcaENFq_i"
      },
      "source": [
        "# First, all images need to be greyscale\n",
        "grey_images = []\n",
        "for x in training_set:\n",
        "  grey_images.append(np.uint8(np.mean(x, axis=-1)))     # reduce all images to single axis by averaging pixel intensities\n",
        "\n",
        "# Then we need to get the average image\n",
        "avg_image = np.average(grey_images, axis = 0)       # and average all those images to get the mean face\n",
        "\n",
        "# And then the average markers\n",
        "avg_pts = []\n",
        "for x in range(46):                          # for each of the markers\n",
        "  px = np.average(training_set_pts[:,x,0])    # average every x coord for that marker\n",
        "  py = np.average(training_set_pts[:,x,1])    # average every y coord for that marker\n",
        "  temp = [px, py]\n",
        "  avg_pts.append(temp) \n",
        "\n",
        "def pre_processing(image):\n",
        "  # image = np.uint8(np.mean(image, axis=-1))        # greyscale       # removed because it made it run worse?\n",
        "  # norm_img = np.zeros((236,236))\n",
        "  # norm_img = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX) # seems to have no effect and just increases run time\n",
        "  blur_img = cv2.GaussianBlur(image,(5,5),0)\n",
        "  resized_image = resize_image(blur_img)\n",
        "  return resized_image\n",
        "\n",
        "def resize_image(image):\n",
        "  return(cv2.resize(image, (118,118), interpolation= cv2.INTER_AREA))   # reducing size to 118 (scaling factor of 0.5)\n",
        "\n",
        "def resize_pts(pts):\n",
        "  resized_pts = []\n",
        "  for x in pts:                          # for each point, half it's coords\n",
        "    rx = x[0] / 2\n",
        "    ry = x[1] / 2\n",
        "    resized_pts.append([rx, ry]) # append in the correct format\n",
        "  return resized_pts\n",
        "\n",
        "def upscale(pts):\n",
        "  resized_pts = []           \n",
        "  for x in pts:         # for each point, double it's coords\n",
        "    rx = x[0] * 2\n",
        "    ry = x[1] * 2\n",
        "    resized_pts.append([rx, ry])\n",
        "  return resized_pts\n",
        "\n",
        "visualise_pts(avg_image, avg_pts, 'gray')\n",
        "cv2_imshow(pre_processing(training_set[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNz9a3_IhATx"
      },
      "source": [
        "# To get more training data, we're gonna flip all the images. ### NO LONGER IN USE ###\n",
        "#flipped_images = []\n",
        "#flipped_pts = []\n",
        "\n",
        "#for x in range(len(training_set)):\n",
        "#  flipped_images.append(np.fliplr(np.copy(training_set[x])))\n",
        "\n",
        "#flipped_images = np.array(flipped_images)  # has to be a numpy array becuase apparently lists just aren't good enough -_-\n",
        "\n",
        "#for x in range(len(training_set_pts)):\n",
        "#  retPts = []\n",
        "#  ys = []                                   \n",
        "#  xs = []\n",
        "#  for p in training_set_pts[x]:\n",
        "#    ys.append(p[1])                 # y coords don't need to change\n",
        "#    xs.append(236 - p[0])           # invert the x coords\n",
        "#  for y in range(len(xs)):\n",
        "#    retPts.append([xs[y],ys[y]])    # and correctly reformat\n",
        "#  flipped_pts.append(retPts)\n",
        "\n",
        "#actually_flipped_pts = []\n",
        "#for x in flipped_pts:\n",
        "#   actually_flipped_pts.append(reorderList(x))    # and reorder them.\n",
        "\n",
        "#actually_flipped_pts = np.array(actually_flipped_pts)        # and turn this one into a numpy array too\n",
        "\n",
        "#training_set_pts = np.concatenate((training_set_pts, actually_flipped_pts))        ##\n",
        "#training_set = np.concatenate((training_set, flipped_images))                      # add both of these too the training set\n",
        "\n",
        "#visualise_pts(flipped_images[1], flipped_pts[1])\n",
        "#visualise_pts(training_set[-1], training_set_pts[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbd8U33DuqGu"
      },
      "source": [
        "#def reorderList(givenList):\n",
        "#  reorder = [6,5,4,3,2,1,0,12,11,10,9,8,7,13,14,17,16,15,27,26,25,24,29,28,21,20,19,18,23,22,36,35,34,33,32,31,30,41,40,39,38,37,44,43,42,45]     # this is the mapping order\n",
        "                                                                                                                                                   # so that these flipped points are\n",
        "                                                                                                                                                   # correctly ordered and labeled\n",
        "#  reorderedList = []\n",
        "#  for i in reorder:\n",
        "#    reorderedList.append(givenList[i])\n",
        "#  return reorderedList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJLIrZgat0_"
      },
      "source": [
        "# Cascaded Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v2wj44padXK"
      },
      "source": [
        "def cascade_train(no_of_regressors, damping_factor):\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  points = []\n",
        "  processed_images = []\n",
        "  resized_ground_points = []  \n",
        "\n",
        "  for x in training_set_pts:                        \n",
        "    temp = np.copy(avg_pts)                           # copying just in case\n",
        "    points.append(resize_pts(temp))                   ##\n",
        "    resized_ground_points.append(resize_pts(x))       # and then we resize the starting points and ground points\n",
        "\n",
        "  for x in training_set:\n",
        "    processed_images.append(pre_processing(x))        # preprocessing\n",
        "\n",
        "  points = np.array(points)\n",
        "  resized_ground_points = np.array(resized_ground_points)       # apparently they need to be in arrays.\n",
        "\n",
        "  regressors = []       \n",
        "  for i in range(no_of_regressors):  # for each regressor\n",
        "    A  = []\n",
        "    target = []\n",
        "    for j in range(len(processed_images)): # for each image\n",
        "      if i>=1: \n",
        "        points[j] = regressor_predict(processed_images[j],regressors[i-1], points[j], sift, damping_factor) # we don't want this to run the first itteration as there isn't a previous regressor \n",
        "                                                                                                            # to run yet. Passing the sift feature here makes it a lot faster.\n",
        "    \n",
        "      A.append(sift.compute(processed_images[j],get_keypoints_from_points(points[j]))[1])       # we only want the descriptors - we calculated the keypoints outselves already.\n",
        "      \n",
        "      target.append(resized_ground_points[j]-points[j])\n",
        "\n",
        "    A = np.array([a.flatten() for a in A])                            # the model wants it flattened\n",
        "    target = np.array([t.flatten() for t in target])                  \n",
        "    model = linear_model.LinearRegression()                           \n",
        "    regressors.append(model.fit(A,target))                            # fits data into a linear regressor, courtesy of sklearn\n",
        "    \n",
        "  return regressors\n",
        "\n",
        "def regressor_predict(image,regressor, previous_points, sift, damping_factor = 0.15):\n",
        "  points = previous_points\n",
        "  a = sift.compute(image,get_keypoints_from_points(points))[1]          # once again, we only want the descriptors\n",
        "  prediction = regressor.predict(a.reshape(-1,5888))                    # because we flattened the array, its size is now 5888 (46 features x 128(depth of sift features))\n",
        "  prediction = prediction * damping_factor                              # using a damping factor so that we don't overshoot and start oscilating around the ground points\n",
        "  returnVal = points + (prediction.reshape(-1,2))                       # we want to return points - not deltas\n",
        "  return returnVal                \n",
        "\n",
        "def get_keypoints_from_points(face_points,keypoint_size=1):\n",
        "  keypoints = []\n",
        "  for p in range(46): \n",
        "    x_coord = face_points[p][0]\n",
        "    y_coord = face_points[p][1]\n",
        "    keypoints.append(cv2.KeyPoint(x_coord,y_coord,keypoint_size))\n",
        "  return keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E50Mr0rhbzXM"
      },
      "source": [
        "regressors = cascade_train(25, 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TghPfVHDgY2k"
      },
      "source": [
        "def run_regressors(image, regressors, damping_factor):\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  points = resize_pts(avg_pts)\n",
        "  image = pre_processing(image)\n",
        "  for k in range(len(regressors)):                                                  # going through each regressor\n",
        "    a = sift.compute(image,get_keypoints_from_points(points))[1]                            # calculate sift features\n",
        "    prediction = regressors[k].predict(a.reshape(-1,5888))                          # and make the prediction\n",
        "    prediction = prediction * damping_factor\n",
        "    points = points + (prediction.reshape(-1,2))\n",
        "  return upscale(points)                                                            # we need to remember to upscale the\n",
        "                                                                                    # points at the end so they fit on the\n",
        "                                                                                    # image correctly\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtQP1PcPGZ0j"
      },
      "source": [
        "for x in range(len(validation_set)):\n",
        "  damping_factor = 0.15\n",
        "  new_points = run_regressors(validation_set[x], regressors, damping_factor)\n",
        "  t = np.average(calculate_error(new_points, validation_set_points[x])) \n",
        "  if t < 4:\n",
        "    visualise_pts(validation_set[x], new_points)\n",
        "    print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyHy_Rcs7xGo"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VGokEFDIRuC"
      },
      "source": [
        "### AN EXPERIMENT\n",
        "\n",
        "dampingFactors = [0.001, 0.005, 0.01, 0.05, 0.075, 0.1]\n",
        "numbersOfRegressors = [5, 10, 15, 20, 25]\n",
        "\n",
        "results = {}\n",
        "for factor in dampingFactors:\n",
        "  for regressorCount in numbersOfRegressors:\n",
        "    temp = str(factor) + \"|\" + str(regressorCount)\n",
        "    results[temp] = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for factor in dampingFactors:\n",
        "  for regressorCount in numbersOfRegressors:\n",
        "    count += 1\n",
        "    print(str(count) + \" / 30\" )\n",
        "    regressors = cascade_train(regressorCount, factor)\n",
        "    temp = str(factor) + \"|\" + str(regressorCount)\n",
        "    for im in range(len(validation_set)):\n",
        "      new_points = cascade(validation_set[im], regressors, factor)\n",
        "      results[temp].append(calculateError(new_points, validation_set_points[im]))\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF_v0VOnIsDu"
      },
      "source": [
        "### Second Experiment\n",
        "dampingFactors = [0.1, 0.15, 0.2]\n",
        "numbersOfRegressors = [25, 30, 40, 50]\n",
        "\n",
        "results = {}\n",
        "for factor in dampingFactors:\n",
        "  for regressorCount in numbersOfRegressors:\n",
        "    temp = str(factor) + \"|\" + str(regressorCount)\n",
        "    results[temp] = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for factor in dampingFactors:\n",
        "  for regressorCount in numbersOfRegressors:\n",
        "    count += 1\n",
        "    print(str(count) + \" / 16\" )\n",
        "    regressors = cascade_train(regressorCount, factor)\n",
        "    temp = str(factor) + \"|\" + str(regressorCount)\n",
        "    for im in range(len(validation_set)):\n",
        "      new_points = cascade(validation_set[im], regressors, factor)\n",
        "      results[temp].append(calculateError(new_points, validation_set_points[im]))\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvsYmj-3-qhs"
      },
      "source": [
        "averages = {}\n",
        "\n",
        "for x in results:\n",
        "  averages[x] = np.average(results[x])\n",
        "\n",
        "print(averages)\n",
        "print(sorted(averages))\n",
        "\n",
        "## 0.15 | 25 seems to be the best\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaxiiQnDSwK-"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "_data = {'0.1|25': 168.25347255294443, '0.1|30': 167.4106967820352, '0.1|40': 167.6372564067228, '0.1|50': 167.8401195469865, '0.15|25': 166.91659154202898, '0.15|30': 167.23857593344994, '0.15|40': 167.4855639985779, '0.15|50': 167.5440399544135, '0.2|25': 168.1771384041849, '0.2|30': 168.290513875452, '0.2|40': 168.3411044950288, '0.2|50': 168.34645097543452}\n",
        "_pd = []\n",
        "_indexi = []\n",
        "\n",
        "for x in _data:\n",
        "  _pd.append(np.average(_data[x]))\n",
        "  _indexi.append(x)\n",
        "\n",
        "df = pd.DataFrame(_pd, index=_indexi)\n",
        "df.plot(legend=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvwPNqwnY18h"
      },
      "source": [
        "output_data = []\n",
        "for x in test_images:\n",
        "  temp_points = run_regressors(x, regressors, 0.15)\n",
        "  output_data.append(temp_points)\n",
        "  visualise_pts(x, temp_points)\n",
        "data_to_be_saved = np.array(output_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjUtSAPh75t4"
      },
      "source": [
        "# Flower Crown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dt9VomR7pNn"
      },
      "source": [
        "def applyFlowerCrown(img, pts):\n",
        "  original_image = Image.fromarray(np.uint8(np.array(np.copy(img))))\n",
        "  temple_to_temple = int(pts[6][0] - pts[0][0]) + 60      # this is the difference between points 0 and 6, with an adition of 60 pixels for error cases\n",
        "  height = int(temple_to_temple / 2)                      # the height is half of this to count for scaling\n",
        "\n",
        "  \n",
        "  xDelta = pts[11][0] - pts[8][0]                         # this is some basic trigonometry to figure out the angle the crown needs\n",
        "  yDelta = pts[11][1] - pts[8][1]                         # to rest at to look correct.\n",
        "  opp_over_adj = yDelta / xDelta\n",
        "  angle_between_eyebrows = np.arctan(opp_over_adj) * -57.3       # radians bad\n",
        "\n",
        "\n",
        "  source_x = int(pts[8][0] - (temple_to_temple/3))             # these values have been adjusted and this seems to look best on the test cases i tried\n",
        "  source_y = int(pts[8][1] - (height/1.3))\n",
        "\n",
        "  crown = Image.open('/content/gdrive/MyDrive/flowerCrown.png')  # reimport so reruns are all reset images\n",
        "\n",
        "  crown = crown.resize((temple_to_temple,height))\n",
        "  crown = crown.rotate(angle_between_eyebrows)\n",
        "\n",
        "  original_image.paste(crown,[source_x, source_y], mask=crown)\n",
        "\n",
        "  return original_image\n",
        "\n",
        "#num = 9\n",
        "#applyFlowerCrown(training_set[num], training_set_pts[num])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xuzfeym7913"
      },
      "source": [
        "for ex_im in example_images:\n",
        "  new_points = run_regressors(ex_im, regressors, damping_factor)\n",
        "  display(applyFlowerCrown(ex_im, new_points))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP2spQio78xB"
      },
      "source": [
        "# Save Data to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iqFP5wTbNs-"
      },
      "source": [
        "save_as_csv(data_to_be_saved, \"/content/gdrive/MyDrive/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}